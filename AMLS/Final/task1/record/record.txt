2021-12-14_14-56-27 base
2021-12-14_14-56-38 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))
2021-12-14_18-47-07 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 多加dropout cnn.add(tf.keras.layers.Dropout(0.2))
2021-12-14_19-06-28 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 多加dropout cnn.add(tf.keras.layers.Dropout(0.1))
2021-12-14_20-55-20 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 多加dropout cnn.add(tf.keras.layers.Dropout(0.3))
2021-12-14_21-20-10 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 输出层激活函数换为relu
2021-12-14_21-46-29 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 两个卷积层激活函数换为sigmoid
2021-12-14_22-05-32 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 两个卷积层激活函数换为tanh
2021-12-14_22-25-28 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 两个卷积层激活函数换为softmax
2021-12-15_00-06-06 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 卷积层kernel size从3换为4
2021-12-15_00-46-38 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 卷积层kernel size从3换为2
2021-12-15_01-09-01 多加一全连接层 cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) pooling size从2换4







